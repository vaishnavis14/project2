---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Vaishnavi Sathiyamoorthy vs25229

### Introduction 

Cardiovascular diseases cause the most number of deaths around the world. In fact, it is responsible for approximately 31% of deaths. This dataset includes 11 factors that could help predict the the possibility of an individual having cardiovascular disease. These factors are age, sex, type of chest pain, resting blood pressure, cholesterol, fasting blood sugar, resting electrocardiogram results, maximum heartrate, whether exercise induces angina, the old peak, and the type of slope of the peak during exercise. This dataset contains 918 observations and was found on Kaggle. 410 of the observations have heart disease and 508 of the observations do not have heart disease. The goal of this project is to predict whether an individual has heart disease based on these factors. 

```{R}
library(tidyverse)
heart_data <- read_csv("heart.csv")
head(heart_data) 
```

### Cluster Analysis

```{R fig.height = 10}
library(cluster)
sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(heart_data, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
heart_pam <- heart_data %>% pam(k = 2)
heart_pam$silinfo$avg.width

library(GGally)
heart_data %>% mutate(cluster=as.factor(heart_pam$clustering)) %>% ggpairs(cols = 1:12, aes(color = cluster))
```

The graph shows that the highest silhouette width is present when k = 2. The pam function shows that the average silhouette is 0.7 for 2 clusters. This means that a reasonable structure has been found. The graphs show that those with higher age, resting BP, and old peak as well as lower cholesterol and maximum heart rate tend to have a higher chance for heart disease.
    
    
### Dimensionality Reduction with PCA

```{R}
heart_numeric_data <- heart_data %>% select(Age, RestingBP, Cholesterol, MaxHR, Oldpeak)
pca1 <- princomp(heart_numeric_data, cor = T)
summary(pca1, loadings = T)

library(factoextra)
fviz_pca_biplot(pca1)
```

88% of the variance is accounted for by the first 4 principle components. To simplify the graph, only the first 2 principle components were plotted. Principle component 1 shows that those with a higher age tend to have a higher resting blood pressure, lower cholesterol, lower maximum heart rate, and higher old peaks. Principle component 2 shows that those with a higher cholesterol tend to have a higher resting blood pressure, higher maximum heart rate, and higher old peak.  

###  Linear Classifier

```{R}
logistic_fit <- glm((HeartDisease == 1) ~ Age + RestingBP + Cholesterol + MaxHR + Oldpeak, data = heart_data, family="binomial")

prob_reg <- predict(logistic_fit, new_data = heart_data, type = "response")
class_diag(prob_reg, heart_data$HeartDisease, positive = 1)
```

```{R}
k <- 11
data <- sample_frac(heart_data) #randomly order rows
folds <- rep(1:k, length.out = nrow(data)) #create folds

diags <- NULL

i = 1
for(i in 1:k){
# create training and test sets
train <- data[folds!= i,] 
test <- data[folds == i,] 
truth <- test$HeartDisease

# train model
fit <- glm(HeartDisease ~ Age + RestingBP + Cholesterol + MaxHR + Oldpeak, data = test, family = "binomial")

# test model
probs <- predict(fit, test, type = "response")

# get performance metrics for each fold
diags <- rbind(diags, class_diag(probs, truth, positive = 1)) }
#average performance metrics across all folds
summarize_all(diags, mean)
```

The AUC for the linear model for the whole dataset is 0.83. There is a slight increase to 0.85 when cross validating the data. This is a good AUC because it is between 0.8 and 0.9. The purpose of looking at AUC is to determine how well the factors can distinguish between individuals with cardiovascular disease and those without cardiovascular disease.

### Non-Parametric Classifier

```{R}
library(caret)
knn_fit <- knn3(HeartDisease ~  Age + RestingBP + Cholesterol + MaxHR + Oldpeak, data = heart_data)
prob_knn <- predict(knn_fit, heart_data)[,2]
class_diag(prob_knn, heart_data$HeartDisease, positive = 1)
```

```{R}
k = 11

data <- sample_frac(heart_data) #randomly order rows
folds <- rep(1:k, length.out = nrow(data)) #create folds

diags <- NULL

i = 1
for(i in 1:k){
# create training and test sets
train <- data[folds != i,] 
test <- data[folds == i,] 
truth <- test$HeartDisease

# train model
fit <- knn3(HeartDisease ~ Age + RestingBP + Cholesterol + MaxHR + Oldpeak, data = test)

# test model
probs <- predict(fit, test)[,2]

# get performance metrics for each fold
diags <- rbind(diags, class_diag(probs, truth, positive = 1)) }

#average performance metrics across all folds
summarize_all(diags, mean)
```

The AUC of the whole dataset using k nearest means is 0.87. However, the AUC when cross validating the data is 0.85, which is slightly lower. Thus, there are signs of overfitting. This is a good AUC because it is between 0.8 and 0.9. The AUC is approximately the same when comparing the non-parametric and parametric classifiers.


### Regression/Numeric Prediction

```{R}
fit<-lm(HeartDisease~.,data = heart_data)
yhat<-predict(fit)
mean((heart_data$HeartDisease-yhat)^2)
```

```{R}
k = 11 #choose number of folds
data <- heart_data[sample(nrow(heart_data)),] #randomly order rows
folds <- cut(seq(1:nrow(heart_data)),breaks = k,labels = F) #create folds
diags <- NULL
for(i in 1:k){
  train <- data[folds!= i,]
  test <- data[folds == i,]
  ## Fit linear regression model to training set
  fit <- lm(HeartDisease~.,data = train)
  ## Get predictions/y-hats on test set (fold i)
  yhat <- predict(fit, newdata = test)
  ## Compute prediction error  (MSE) for fold i
  diags <- mean((test$HeartDisease-yhat)^2) 
}
mean(diags)
```

There seems to be a little bit of overfitting since the cross validation set has an average mean squared error of  0.12, which is slightly higher than the regression model for the whole dataset, 0.10.

### Python 

```{R}
library(reticulate)
library(dplyr)
use_python("/usr/bin/python3")
female_heart_data <- heart_data %>% filter(Sex == 'F')
```

```{python}
r.female_heart_data.agg('mean')
r.heart_data.groupby('ChestPainType').agg('mean')
```

The heart dataset that was worked on throughout this project was passed onto Python. The mean for each variable is computed based on chestpain type. The data shows that those with ASY chestpain have a signficantly higher chance of having cardiovascular disease. A new object was created with R code that only had data on females. This was passed onto Python and the mean for each variable was caluculated. Interestingly, the data shows that the probability of a female having heart disease is only 25%.

### Concluding Remarks

Cluster analysis showed that those with higher age, resting BP, and old peak as well as lower cholesterol and maximum heart rate tend to have a higher chance for heart disease. Linear classifying models differentiate those with and without heart disease accurately about 85% of the time. Meanwhile, a non-parametric classifier accurately differentiates those with and without heart disease about 85% of the time. Both AUCs are pretty good. Finally, the regression predictions have a mean squared error of 0.12.

Cardiovascular disease is the leading cause of death around the world. Using various factors to predict whether an individual has heart disease is imperative to decrease fatalities.



